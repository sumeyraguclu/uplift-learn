{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8caa9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… KÃ¼tÃ¼phaneler yÃ¼klendi!\n",
      "ğŸ¯ Toy Dataset Test\n",
      "------------------------------------------------------------\n",
      "         f0        f1        f2        f3        f4  treatment  visit\n",
      "0  0.496714 -0.138264  0.647689  1.523030 -0.234153          1      1\n",
      "1 -0.234137  1.579213  0.767435 -0.469474  0.542560          1      0\n",
      "2 -0.463418 -0.465730  0.241962 -1.913280 -1.724918          0      0\n",
      "3 -0.562288 -1.012831  0.314247 -0.908024 -1.412304          1      1\n",
      "4  1.465649 -0.225776  0.067528 -1.424748 -0.544383          0      1\n",
      "\n",
      "Shape: (5000, 7)\n",
      "âš ï¸  Criteo verisi bulunamadÄ±. Toy dataset ile devam ediyoruz.\n",
      "ğŸ“‹ VERÄ° SETÄ° BÄ°LGÄ°LERÄ°\n",
      "============================================================\n",
      "SatÄ±r sayÄ±sÄ±: 5,000\n",
      "SÃ¼tun sayÄ±sÄ±: 7\n",
      "Bellek kullanÄ±mÄ±: 0.23 MB\n",
      "\n",
      "SÃ¼tunlar:\n",
      "f0           float64\n",
      "f1           float64\n",
      "f2           float64\n",
      "f3           float64\n",
      "f4           float64\n",
      "treatment      int32\n",
      "visit          int32\n",
      "dtype: object\n",
      "\n",
      "ğŸ“Š EKSÄ°K DEÄER ANALÄ°ZÄ°\n",
      "============================================================\n",
      "âœ… Eksik deÄŸer yok!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(missing[missing > \u001b[32m0\u001b[39m])\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# ## ğŸ¯ 4. Treatment/Control Analizi\u001b[39;00m\n\u001b[32m     87\u001b[39m \n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m X, y, t = \u001b[43mget_features_target_treatment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Treatment daÄŸÄ±lÄ±mÄ±\u001b[39;00m\n\u001b[32m     92\u001b[39m check_treatment_balance(t, \u001b[33m\"\u001b[39m\u001b[33mFull Dataset\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sumeyra\\dev\\uplift-learn\\notebooks\\..\\src\\data.py:111\u001b[39m, in \u001b[36mget_features_target_treatment\u001b[39m\u001b[34m(df, features, target, treatment)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# VarsayÄ±lan: f0-f11\u001b[39;00m\n\u001b[32m    109\u001b[39m     features = [\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mf\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m12\u001b[39m)]\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m X = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m.copy()\n\u001b[32m    112\u001b[39m y = df[target].copy()\n\u001b[32m    113\u001b[39m t = df[treatment].copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sumeyra\\dev\\uplift-learn\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3897\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   3898\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m3899\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   3901\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   3902\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sumeyra\\dev\\uplift-learn\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6112\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6113\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6115\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6117\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6119\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sumeyra\\dev\\uplift-learn\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6179\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6176\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6178\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6179\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11'] not in index\""
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # ğŸ“Š GÃ¼n 1: Veri KeÅŸfi ve HazÄ±rlÄ±k\n",
    "# \n",
    "# **Hedefler**:\n",
    "# 1. Criteo uplift veri setini anlamak\n",
    "# 2. Treatment/Control gruplarÄ±nÄ± incelemek\n",
    "# 3. Baseline metrikleri hesaplamak\n",
    "# 4. Veri kalitesini kontrol etmek\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ“¥ 1. KÃ¼tÃ¼phaneleri ve Veriyi YÃ¼kle\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Kendi modÃ¼lÃ¼mÃ¼z\n",
    "from src.data import (\n",
    "    load_criteo_sample,\n",
    "    get_features_target_treatment,\n",
    "    train_test_split_uplift,\n",
    "    check_treatment_balance,\n",
    "    calculate_baseline_metrics,\n",
    "    create_toy_dataset\n",
    ")\n",
    "\n",
    "# GÃ¶rselleÅŸtirme ayarlarÄ±\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… KÃ¼tÃ¼phaneler yÃ¼klendi!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ“¦ 2. Veri Setini YÃ¼kle\n",
    "\n",
    "# %%\n",
    "# Ã–nce toy dataset ile test edelim\n",
    "print(\"ğŸ¯ Toy Dataset Test\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "df_toy = create_toy_dataset(n_samples=5000, treatment_effect_size=0.15)\n",
    "print(df_toy.head())\n",
    "print(f\"\\nShape: {df_toy.shape}\")\n",
    "\n",
    "# %%\n",
    "# Criteo veri setini yÃ¼kle (eÄŸer hazÄ±rsa)\n",
    "try:\n",
    "    df = load_criteo_sample(\"../data/criteo_sample.parquet\")\n",
    "    print(\"âœ… Criteo veri seti yÃ¼klendi!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸  Criteo verisi bulunamadÄ±. Toy dataset ile devam ediyoruz.\")\n",
    "    df = df_toy\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ” 3. Veri YapÄ±sÄ±nÄ± Ä°ncele\n",
    "\n",
    "# %%\n",
    "print(\"ğŸ“‹ VERÄ° SETÄ° BÄ°LGÄ°LERÄ°\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"SatÄ±r sayÄ±sÄ±: {len(df):,}\")\n",
    "print(f\"SÃ¼tun sayÄ±sÄ±: {len(df.columns)}\")\n",
    "print(f\"Bellek kullanÄ±mÄ±: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nSÃ¼tunlar:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# %%\n",
    "# Eksik deÄŸer kontrolÃ¼\n",
    "print(\"\\nğŸ“Š EKSÄ°K DEÄER ANALÄ°ZÄ°\")\n",
    "print(\"=\" * 60)\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"âœ… Eksik deÄŸer yok!\")\n",
    "else:\n",
    "    print(missing[missing > 0])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ¯ 4. Treatment/Control Analizi\n",
    "\n",
    "# %%\n",
    "X, y, t = get_features_target_treatment(df)\n",
    "\n",
    "# Treatment daÄŸÄ±lÄ±mÄ±\n",
    "check_treatment_balance(t, \"Full Dataset\")\n",
    "\n",
    "# %%\n",
    "# GÃ¶rselleÅŸtirme\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Treatment distribution\n",
    "axes[0].bar(['Control', 'Treatment'], \n",
    "            [np.sum(t==0), np.sum(t==1)],\n",
    "            color=['#ff6b6b', '#4ecdc4'])\n",
    "axes[0].set_title('Treatment Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, v in enumerate([np.sum(t==0), np.sum(t==1)]):\n",
    "    axes[0].text(i, v + 50, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Outcome distribution by treatment\n",
    "outcome_by_treatment = pd.DataFrame({\n",
    "    'Control': [np.sum((t==0) & (y==0)), np.sum((t==0) & (y==1))],\n",
    "    'Treatment': [np.sum((t==1) & (y==0)), np.sum((t==1) & (y==1))]\n",
    "}, index=['No Visit', 'Visit'])\n",
    "\n",
    "outcome_by_treatment.plot(kind='bar', ax=axes[1], color=['#ff6b6b', '#4ecdc4'])\n",
    "axes[1].set_title('Outcome Distribution by Treatment', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
    "axes[1].legend(title='Group')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ“ˆ 5. Baseline Metrikleri\n",
    "\n",
    "# %%\n",
    "metrics = calculate_baseline_metrics(y, t)\n",
    "\n",
    "# %%\n",
    "# Conversion rates gÃ¶rselleÅŸtirme\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "cr_control = metrics['conversion_rate_control']\n",
    "cr_treatment = metrics['conversion_rate_treatment']\n",
    "\n",
    "bars = ax.bar(['Control', 'Treatment'], \n",
    "              [cr_control, cr_treatment],\n",
    "              color=['#ff6b6b', '#4ecdc4'],\n",
    "              edgecolor='black',\n",
    "              linewidth=2)\n",
    "\n",
    "ax.set_ylabel('Conversion Rate', fontsize=12)\n",
    "ax.set_title('Conversion Rates: Treatment vs Control', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, max(cr_control, cr_treatment) * 1.2)\n",
    "\n",
    "# DeÄŸerleri gÃ¶ster\n",
    "for i, (bar, val) in enumerate(zip(bars, [cr_control, cr_treatment])):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "            f'{val:.4f}\\n({val:.2%})',\n",
    "            ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# ATE Ã§izgisi\n",
    "ax.axhline(y=cr_control, color='red', linestyle='--', alpha=0.5, label=f'ATE = {metrics[\"ate\"]:.4f}')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ¯ KEY INSIGHT:\")\n",
    "print(f\"   Treatment increases conversion by {metrics['relative_uplift']:+.2%}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ”¬ 6. Feature Distribution Analizi\n",
    "\n",
    "# %%\n",
    "print(\"ğŸ“Š Ã–ZELLÄ°K Ä°STATÄ°STÄ°KLERÄ°\")\n",
    "print(\"=\" * 60)\n",
    "print(X.describe())\n",
    "\n",
    "# %%\n",
    "# Feature distributions (ilk 6 feature)\n",
    "n_features = min(6, X.shape[1])\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(n_features):\n",
    "    feature_name = X.columns[i]\n",
    "    \n",
    "    # Treatment ve Control iÃ§in ayrÄ± histogramlar\n",
    "    axes[i].hist(X[t==0][feature_name], bins=30, alpha=0.5, \n",
    "                 label='Control', color='#ff6b6b', density=True)\n",
    "    axes[i].hist(X[t==1][feature_name], bins=30, alpha=0.5, \n",
    "                 label='Treatment', color='#4ecdc4', density=True)\n",
    "    \n",
    "    axes[i].set_title(f'{feature_name} Distribution', fontweight='bold')\n",
    "    axes[i].set_xlabel(feature_name)\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Feature Distributions: Treatment vs Control', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## âš–ï¸ 7. Covariate Balance Check\n",
    "# \n",
    "# **Ã–nemli**: RCT'de (Randomized Controlled Trial) treatment ve control gruplarÄ± \n",
    "# Ã¶zellikler aÃ§Ä±sÄ±ndan dengeli olmalÄ±dÄ±r. Aksi halde, tedavi etkisini doÄŸru Ã¶lÃ§emeyiz.\n",
    "\n",
    "# %%\n",
    "def check_covariate_balance(X, treatment, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Her Ã¶zellik iÃ§in treatment/control gruplarÄ± arasÄ±nda t-test yap\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for col in X.columns:\n",
    "        x_treatment = X[treatment == 1][col]\n",
    "        x_control = X[treatment == 0][col]\n",
    "        \n",
    "        # T-test\n",
    "        t_stat, p_value = stats.ttest_ind(x_treatment, x_control)\n",
    "        \n",
    "        # Standardized mean difference (SMD)\n",
    "        mean_diff = x_treatment.mean() - x_control.mean()\n",
    "        pooled_std = np.sqrt((x_treatment.var() + x_control.var()) / 2)\n",
    "        smd = mean_diff / pooled_std if pooled_std > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'Feature': col,\n",
    "            'Mean (Treatment)': x_treatment.mean(),\n",
    "            'Mean (Control)': x_control.mean(),\n",
    "            'Difference': mean_diff,\n",
    "            'SMD': smd,\n",
    "            't-statistic': t_stat,\n",
    "            'p-value': p_value,\n",
    "            'Balanced': 'âœ…' if p_value > alpha else 'âš ï¸'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# %%\n",
    "balance_df = check_covariate_balance(X, t)\n",
    "print(\"\\nâš–ï¸  COVARIATE BALANCE TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(balance_df.to_string(index=False))\n",
    "\n",
    "# Ã–zet\n",
    "n_balanced = (balance_df['p-value'] > 0.05).sum()\n",
    "print(f\"\\nğŸ“Š Summary: {n_balanced}/{len(balance_df)} features are balanced (p > 0.05)\")\n",
    "\n",
    "if n_balanced == len(balance_df):\n",
    "    print(\"âœ… Perfect balance! Randomization worked well.\")\n",
    "else:\n",
    "    print(\"âš ï¸  Some imbalance detected. Consider using propensity score weighting.\")\n",
    "\n",
    "# %%\n",
    "# SMD gÃ¶rselleÅŸtirmesi\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['green' if abs(smd) < 0.1 else 'orange' if abs(smd) < 0.2 else 'red' \n",
    "          for smd in balance_df['SMD']]\n",
    "\n",
    "ax.barh(balance_df['Feature'], balance_df['SMD'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "ax.axvline(x=0.1, color='orange', linestyle='--', alpha=0.5, label='SMD = 0.1')\n",
    "ax.axvline(x=-0.1, color='orange', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=0.2, color='red', linestyle='--', alpha=0.5, label='SMD = 0.2')\n",
    "ax.axvline(x=-0.2, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Standardized Mean Difference (SMD)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Covariate Balance: SMD Plot', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Interpretation:\")\n",
    "print(\"   |SMD| < 0.1: Excellent balance\")\n",
    "print(\"   |SMD| < 0.2: Acceptable balance\")\n",
    "print(\"   |SMD| â‰¥ 0.2: Concerning imbalance\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ“Š 8. Outcome Analysis by Feature Quantiles\n",
    "# \n",
    "# Ã–zelliklerin outcome Ã¼zerindeki etkisini gÃ¶rmek iÃ§in quantile analizi yapalÄ±m.\n",
    "\n",
    "# %%\n",
    "def analyze_feature_quantiles(X, y, treatment, feature_name, n_quantiles=5):\n",
    "    \"\"\"\n",
    "    Bir Ã¶zelliÄŸin quantile'larÄ±na gÃ¶re conversion rate analizi\n",
    "    \"\"\"\n",
    "    feature_data = X[feature_name]\n",
    "    \n",
    "    # Quantile'larÄ± oluÅŸtur\n",
    "    quantiles = pd.qcut(feature_data, q=n_quantiles, labels=False, duplicates='drop')\n",
    "    \n",
    "    results = []\n",
    "    for q in range(n_quantiles):\n",
    "        mask_q = (quantiles == q)\n",
    "        \n",
    "        # Treatment grubu\n",
    "        mask_t = mask_q & (treatment == 1)\n",
    "        cr_t = y[mask_t].mean() if mask_t.sum() > 0 else np.nan\n",
    "        \n",
    "        # Control grubu\n",
    "        mask_c = mask_q & (treatment == 0)\n",
    "        cr_c = y[mask_c].mean() if mask_c.sum() > 0 else np.nan\n",
    "        \n",
    "        # Uplift\n",
    "        uplift = cr_t - cr_c\n",
    "        \n",
    "        results.append({\n",
    "            'Quantile': q + 1,\n",
    "            'Feature Range': f\"[{feature_data[mask_q].min():.2f}, {feature_data[mask_q].max():.2f}]\",\n",
    "            'CR (Treatment)': cr_t,\n",
    "            'CR (Control)': cr_c,\n",
    "            'Uplift': uplift,\n",
    "            'N': mask_q.sum()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# %%\n",
    "# Ä°lk feature iÃ§in analiz\n",
    "first_feature = X.columns[0]\n",
    "quantile_df = analyze_feature_quantiles(X, y, t, first_feature, n_quantiles=5)\n",
    "\n",
    "print(f\"\\nğŸ“Š QUANTILE ANALYSIS: {first_feature}\")\n",
    "print(\"=\" * 80)\n",
    "print(quantile_df.to_string(index=False))\n",
    "\n",
    "# %%\n",
    "# GÃ¶rselleÅŸtirme\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Conversion rates\n",
    "x_pos = quantile_df['Quantile']\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x_pos - width/2, quantile_df['CR (Control)'], width, \n",
    "            label='Control', color='#ff6b6b', alpha=0.8)\n",
    "axes[0].bar(x_pos + width/2, quantile_df['CR (Treatment)'], width, \n",
    "            label='Treatment', color='#4ecdc4', alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Quantile', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Conversion Rate', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title(f'Conversion Rates by {first_feature} Quantile', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Uplift\n",
    "axes[1].bar(x_pos, quantile_df['Uplift'], color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Quantile', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Uplift', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title(f'Uplift by {first_feature} Quantile', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ INSIGHT:\")\n",
    "print(f\"   Uplift varies across quantiles â†’ Heterogeneous treatment effect!\")\n",
    "print(f\"   This is why we need CATE (Conditional ATE) models like T-Learner.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ¯ 9. Train/Test Split\n",
    "\n",
    "# %%\n",
    "X_train, X_test, y_train, y_test, t_train, t_test = train_test_split_uplift(\n",
    "    df, \n",
    "    test_size=0.25, \n",
    "    random_state=42,\n",
    "    stratify_treatment=True\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“¦ TRAIN/TEST SPLIT TAMAMLANDI\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training set size:   {len(X_train):>8,}\")\n",
    "print(f\"Test set size:       {len(X_test):>8,}\")\n",
    "\n",
    "# %%\n",
    "# Balance kontrolÃ¼\n",
    "check_treatment_balance(t_train, \"Training Set\")\n",
    "check_treatment_balance(t_test, \"Test Set\")\n",
    "\n",
    "# %%\n",
    "# Train ve test baseline metrikleri\n",
    "print(\"\\nğŸ‹ï¸ TRAINING SET METRICS\")\n",
    "train_metrics = calculate_baseline_metrics(y_train, t_train)\n",
    "\n",
    "print(\"\\nğŸ§ª TEST SET METRICS\")\n",
    "test_metrics = calculate_baseline_metrics(y_test, t_test)\n",
    "\n",
    "# KarÅŸÄ±laÅŸtÄ±rma\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['CR (Treatment)', 'CR (Control)', 'ATE', 'Relative Uplift'],\n",
    "    'Train': [\n",
    "        train_metrics['conversion_rate_treatment'],\n",
    "        train_metrics['conversion_rate_control'],\n",
    "        train_metrics['ate'],\n",
    "        train_metrics['relative_uplift']\n",
    "    ],\n",
    "    'Test': [\n",
    "        test_metrics['conversion_rate_treatment'],\n",
    "        test_metrics['conversion_rate_control'],\n",
    "        test_metrics['ate'],\n",
    "        test_metrics['relative_uplift']\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison['Difference'] = comparison['Test'] - comparison['Train']\n",
    "\n",
    "print(\"\\nğŸ“Š TRAIN vs TEST COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ’¾ 10. Veriyi Kaydet\n",
    "\n",
    "# %%\n",
    "# EÄŸitim iÃ§in hazÄ±r veriyi kaydet\n",
    "import pickle\n",
    "\n",
    "processed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    't_train': t_train,\n",
    "    't_test': t_test,\n",
    "    'train_metrics': train_metrics,\n",
    "    'test_metrics': test_metrics\n",
    "}\n",
    "\n",
    "output_path = '../data/processed_data.pkl'\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "print(f\"\\nâœ… Ä°ÅŸlenmiÅŸ veri kaydedildi: {output_path}\")\n",
    "print(f\"   Boyut: {len(pickle.dumps(processed_data)) / 1024**2:.2f} MB\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ“ 11. Ã–zet ve Sonraki AdÄ±mlar\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ‰ GÃœN 1 TAMAMLANDI: VERÄ° KEÅFÄ° VE HAZIRLAMA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ… Tamamlanan GÃ¶revler:\")\n",
    "print(\"   1. Veri seti yÃ¼klendi ve incelendi\")\n",
    "print(\"   2. Treatment/Control dengesi kontrol edildi\")\n",
    "print(\"   3. Baseline metrikleri hesaplandÄ±\")\n",
    "print(\"   4. Covariate balance analizi yapÄ±ldÄ±\")\n",
    "print(\"   5. Feature-outcome iliÅŸkileri incelendi\")\n",
    "print(\"   6. Train/test split oluÅŸturuldu\")\n",
    "print(\"   7. Ä°ÅŸlenmiÅŸ veri kaydedildi\")\n",
    "\n",
    "print(\"\\nğŸ“Š Ana Bulgular:\")\n",
    "print(f\"   â€¢ Toplam Ã¶rnek sayÄ±sÄ±: {len(df):,}\")\n",
    "print(f\"   â€¢ Treatment ratio: {t.mean():.2%}\")\n",
    "print(f\"   â€¢ Baseline ATE: {metrics['ate']:.4f} ({metrics['relative_uplift']:+.2%})\")\n",
    "print(f\"   â€¢ Covariate balance: {n_balanced}/{len(balance_df)} features balanced\")\n",
    "\n",
    "print(\"\\nğŸ¯ Sonraki AdÄ±m (GÃ¼n 2):\")\n",
    "print(\"   â†’ T-Learner modelini implement edin\")\n",
    "print(\"   â†’ Ä°ki ayrÄ± XGBoost modeli eÄŸitin\")\n",
    "print(\"   â†’ Uplift tahminleri oluÅŸturun\")\n",
    "print(\"   â†’ Notebook: 02_t_learner_basics.ipynb\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ğŸ”– Notlar ve Ã–ÄŸrenilenler\n",
    "# \n",
    "# ### ğŸ“ Temel Kavramlar\n",
    "# \n",
    "# 1. **Treatment Effect (Tedavi Etkisi)**:\n",
    "#    - ATE = E[Y|T=1] - E[Y|T=0]\n",
    "#    - Ortalama etki, ama bireysel etkiler farklÄ± olabilir!\n",
    "# \n",
    "# 2. **Randomization Check**:\n",
    "#    - RCT'de gruplar dengeli olmalÄ±\n",
    "#    - Covariate balance kritik\n",
    "#    - SMD < 0.1 idealdir\n",
    "# \n",
    "# 3. **Heterogeneous Effects**:\n",
    "#    - Uplift Ã¶zellik deÄŸerlerine gÃ¶re deÄŸiÅŸir\n",
    "#    - Bu yÃ¼zden CATE modelleri gerekli (T-Learner, S-Learner, X-Learner)\n",
    "# \n",
    "# ### ğŸ’¡ Pratik Ä°puÃ§larÄ±\n",
    "# \n",
    "# - Treatment dengesini her zaman kontrol edin\n",
    "# - Train/test split'te stratify kullanÄ±n\n",
    "# - Baseline metrikleri mutlaka hesaplayÄ±n\n",
    "# - Feature distributions'Ä± gÃ¶rselleÅŸtirin\n",
    "# \n",
    "# ### ğŸ“š Kaynaklar\n",
    "# \n",
    "# - Radcliffe & Surry (2007): \"Using control groups to target on predicted lift\"\n",
    "# - KÃ¼nzel et al. (2019): \"Metalearners for estimating heterogeneous treatment effects\"\n",
    "# - scikit-uplift docs: https://www.uplift-modeling.com/\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
